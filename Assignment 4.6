Q1-If 7TB is the available disk space per node (9 disks with 1 TB, 2 disk for operating system etc. were excluded.). Assuming initial data size is 600 TB. How  
will you estimate the number of data nodes (n)?

ANSWER:
The formula for calculating HDFS storage(H) is :
H=C*R*S/(1-i)*120
C = Compression ratio. here C=1 as nothing is mentioned with respect to compression.
R = Replication factor. The default value of R is 3.Thus, R=3.
S=initial size of data. S=600 TB
i=intermediate data factor. This space is required to store results of mapping tasks and any other processing. Cloudera has recommended a 25 % for intermediate data.
i=1/4
120 % : Here, it is already mentioned that 2 disks are used for os.Therefore, this factor can be ignored in the calaculation.
H=1*3*S/(1-3/4)
H=4*S
The number of data nodes(n) can be calculated by the following formula:
n=H/d
Where d=available disk space available per node(d=7 TB)
n=4*S/d
n=4*600/7
n=343(approx) 
The number of data nodes will be 343.

Q2-Imagine that you are uploading a file of 500MB into HDFS.100MB of data is successfully uploaded into HDFS and another client wants to read the
uploaded data while the upload is still in progress. What will happen in such a scenario, will the 100 MB of data that is uploaded will it be displayed?

ANSWER:

The number of blocks will be - N=500/128 = 4
The default size of block in HDFS is 128 MB.
So Simultaneous read and write operations on same data is not permitted as it can compromise data integrity.
Hence, the data will not be displayed. 


